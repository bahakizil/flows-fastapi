# Flowise FastAPI Backend

A powerful, modular FastAPI-based backend for building AI/LLM workflows with a visual interface, inspired by Flowise but built with a focus on developer experience and extensibility.

## ğŸš€ Features

- **Standardized Node Architecture**: Three-tier node system (Provider, Processor, Terminator)
- **Dynamic Node Discovery**: Automatic detection and registration of new nodes
- **LangChain Integration**: Full compatibility with LangChain ecosystem
- **Modular Design**: Easy to extend with custom nodes
- **Type-Safe**: Built with Pydantic for robust data validation
- **RESTful API**: Clean, documented endpoints for frontend integration

## ğŸ“ Project Structure

```
flowise-fastapi/
â”œâ”€â”€ api/                    # FastAPI routes and schemas
â”‚   â”œâ”€â”€ routers/           # API route handlers
â”‚   â””â”€â”€ schemas.py         # Pydantic models
â”œâ”€â”€ core/                  # Core system components
â”‚   â”œâ”€â”€ config.py          # Configuration settings
â”‚   â”œâ”€â”€ node_discovery.py  # Automatic node discovery
â”‚   â””â”€â”€ workflow_runner.py # Workflow execution engine
â”œâ”€â”€ nodes/                 # Node implementations
â”‚   â”œâ”€â”€ base.py           # Base node classes
â”‚   â”œâ”€â”€ llms/             # Language models
â”‚   â”œâ”€â”€ tools/            # External tools
â”‚   â”œâ”€â”€ agents/           # AI agents
â”‚   â”œâ”€â”€ memory/           # Conversation memory
â”‚   â”œâ”€â”€ prompts/          # Prompt templates
â”‚   â”œâ”€â”€ output_parsers/   # Output formatting
â”‚   â”œâ”€â”€ document_loaders/ # Document processing
â”‚   â””â”€â”€ retrievers/       # Information retrieval
â”œâ”€â”€ main.py               # FastAPI application entry point
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ readme-gemini.md      # Detailed project memory/documentation
```

## ğŸ›  Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/bahakizil/fastapi-reactflow.git
   cd fastapi-reactflow
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables:**
   ```bash
   export OPENAI_API_KEY="your-openai-api-key"
   export GOOGLE_API_KEY="your-google-api-key"  # Optional, for Gemini
   export TAVILY_API_KEY="your-tavily-api-key"  # Optional, for Tavily search
   export LANGCHAIN_API_KEY="your-langchain-api-key"  # Optional, for LangSmith tracing
   ```

## ğŸš€ Quick Start

1. **Start the server:**
   ```bash
   uvicorn main:app --reload
   ```

2. **Visit the API documentation:**
   - Interactive docs: http://localhost:8000/docs
   - ReDoc: http://localhost:8000/redoc

3. **List available nodes:**
   ```bash
   curl http://localhost:8000/api/v1/nodes
   ```

## ğŸ“Š Available Nodes

### Provider Nodes (Create LangChain objects)
- **OpenAIChat**: OpenAI GPT models
- **GoogleGemini**: Google Gemini models (requires langchain-google-genai)
- **TavilySearch**: Web search via Tavily API
- **GoogleSearchTool**: Google search via SerpAPI
- **WikipediaTool**: Wikipedia search
- **PromptTemplate**: Chat prompt templates
- **AgentPrompt**: ReAct agent prompts
- **ConversationMemory**: Conversation buffer memory
- **PDFLoader**: PDF document loader

### Processor Nodes (Combine multiple objects)
- **ReactAgent**: ReAct reasoning agent
- **ChromaRetriever**: Vector store retrieval

### Terminator Nodes (Process outputs)
- **StringOutputParser**: Basic string output
- **PydanticOutputParser**: Structured output parsing

## ğŸ”§ API Endpoints

### Nodes
- `GET /api/v1/nodes` - List all available nodes
- `GET /api/v1/nodes/{node_name}` - Get specific node details

### Workflows
- `POST /api/v1/workflows/execute` - Execute a workflow

### Example Workflow Execution
```json
{
  "workflow": {
    "id": "example-workflow",
    "name": "Simple Chat",
    "nodes": [
      {
        "id": "llm-1",
        "type": "OpenAIChat",
        "data": {
          "inputs": {
            "model_name": "gpt-4o-mini",
            "temperature": 0.7
          }
        }
      }
    ],
    "edges": []
  },
  "input": {
    "input": "Hello, how are you?"
  }
}
```

## ğŸ— Node Architecture

### Creating Custom Nodes

1. **Provider Node Example:**
```python
from nodes.base import ProviderNode, NodeInput, NodeType
from langchain_core.runnables import Runnable

class MyCustomNode(ProviderNode):
    _metadatas = {
        "name": "MyCustomNode",
        "description": "Description of what this node does",
        "node_type": NodeType.PROVIDER,
        "inputs": [
            NodeInput(
                name="param1",
                type="string",
                description="Parameter description",
                required=True
            )
        ]
    }
    
    def _execute(self, param1: str = None) -> Runnable:
        # Your implementation here
        return your_langchain_object
```

2. **Place the file in the appropriate `nodes/` subdirectory**
3. **Restart the server** - the node will be automatically discovered

## ğŸ” Node Types

- **ProviderNode**: Creates LangChain objects from scratch (LLMs, Tools, Prompts)
- **ProcessorNode**: Combines multiple LangChain objects (Agents, Chains)
- **TerminatorNode**: Processes final outputs (Parsers, Formatters)

## ğŸ“‹ Development Status

- âœ… Core architecture and node system
- âœ… Dynamic node discovery
- âœ… Workflow execution engine
- âœ… RESTful API endpoints
- âœ… 12 working nodes across all categories
- âš ï¸ Frontend integration (planned)
- âš ï¸ WebSocket streaming (planned)
- âš ï¸ Database persistence (planned)


**Ready to build powerful AI workflows? Start exploring!** ğŸš€